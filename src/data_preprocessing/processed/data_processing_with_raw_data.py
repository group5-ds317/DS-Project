# -*- coding: utf-8 -*-
"""Data Processing With Raw Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_mVao3qeiBtMW8Ut9yuFU7kNkPyACCRA
"""

import pandas as pd
import numpy as np

!pip install transformers

from transformers import AutoTokenizer, AutoModel
import torch

from google.colab import drive
drive.mount("/content/drive")

sinhvien_df = pd.read_excel("/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Dataset/Raw/01.sinhvien.xlsx")
diem_df = pd.read_excel("/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Dataset/Raw/02.diem.xlsx")
totnghiep_df = pd.read_excel("/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Dataset/Raw/14.totnghiep.xlsx")

"""### Nối bảng dữ liệu"""

sinhvien_totnghiep_df = pd.merge(totnghiep_df, sinhvien_df, on='mssv', how='left')

diem_sinhvien_totnghiep_df = pd.merge(sinhvien_totnghiep_df, diem_df, on='mssv', how='inner')

"""### Chọn thuộc tính"""

feature_selection_df = diem_sinhvien_totnghiep_df[['mssv', ' gioitinh', ' khoa', ' hedt', ' mamh', ' sotc', ' diem', ' trangthai', ' namhoc', ' hocky']]

feature_selection_df = feature_selection_df.rename(columns={' gioitinh': 'gioitinh', ' khoa': 'khoa', ' hedt': 'hedt', ' mamh': 'mamh', ' sotc': 'sotc', ' diem': 'diem', ' trangthai': 'trangthai', ' namhoc': 'namhoc', ' hocky': 'hocky'})

"""### Lọc dữ liệu"""

filtered_df = feature_selection_df[feature_selection_df['trangthai'].isin([1, 2, 3])]

unique_df = filtered_df.drop_duplicates(subset=['mssv', 'namhoc', 'hocky', 'mamh'])

"""### Gộp dữ liệu kỳ 3 vào kỳ 2"""

diem_term_1 = unique_df[unique_df['hocky'] == 1]
diem_term_2 = unique_df[unique_df['hocky'] == 2]
diem_term_3 = unique_df[unique_df['hocky'] == 3]

diem_term_2_3 = pd.merge(diem_term_2, diem_term_3, on=['mssv', 'namhoc', 'mamh'], how='left')[['mssv', 'gioitinh_x', 'khoa_x', 'hedt_x', 'mamh', 'sotc_x', 'diem_x', 'diem_y', 'trangthai_x', 'namhoc', 'hocky_x']]

diem_term_2_3['diem'] = diem_term_2_3.apply(lambda x: x['diem_y'] if not pd.isna(x['diem_y']) else x['diem_x'], axis=1)
diem_term_2_3 = diem_term_2_3.drop(columns=['diem_x', 'diem_y'])
diem_term_2_3 = diem_term_2_3.rename(columns={'gioitinh_x': 'gioitinh', 'khoa_x': 'khoa', 'hedt_x': 'hedt', 'sotc_x': 'sotc', 'trangthai_x': 'trangthai', 'hocky_x': 'hocky'})
diem_df = pd.concat([diem_term_1, diem_term_2_3])

"""### Mở rộng dữ liệu"""

subject_categories = pd.read_excel("/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Dataset/Augmented/Danh-muc-mon-hoc.xlsx")
subject_description = pd.read_excel("/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Dataset/Augmented/paraphrased_tomtat.xlsx")

filtered_df['mamh'] = filtered_df['mamh'].astype(str)
subject_categories['Mã MH'] = subject_categories['Mã MH'].astype(str)
filtered_df['mamh'] = filtered_df['mamh'].str.strip()
subject_categories['Mã MH'] = subject_categories['Mã MH'].str.strip()

augmented_df = pd.merge(filtered_df, subject_categories, left_on='mamh', right_on='Mã MH', how='left')
augmented_df = augmented_df[list(filtered_df.columns) + ['Đơn vị quản lý chuyên môn', 'Loại MH']]

augmented_df = augmented_df.rename(columns={'Đơn vị quản lý chuyên môn': 'nganhmh', 'Loại MH': 'loaimh'})

augmented_df = pd.merge(augmented_df, subject_description, on='mamh', how='left')
augmented_df = augmented_df[list(filtered_df.columns) + ['nganhmh', 'loaimh', 'tomtat', 'Paraphrased_Text']]

augmented_df = augmented_df.rename(columns={'Paraphrased_Text': 'paraphased_tomtat'})

"""### Lọc dữ liệu"""

filtered_augmented_df = augmented_df[augmented_df['loaimh'] != 'BT']

"""### Word Embedding dữ liệu tomtat

1. PhoBERT
"""

tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")
model = AutoModel.from_pretrained("vinai/phobert-base")

def phobert_embedding(text):
  inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
  outputs = model(**inputs)
  embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()
  return embeddings

course_paraphased_tomtat = filtered_augmented_df[['mamh', 'paraphased_tomtat']].drop_duplicates(subset=['mamh']).reset_index(drop=True)

PhoBERT_paraphased_tomtat = course_paraphased_tomtat.copy()
PhoBERT_paraphased_tomtat['paraphased_tomtat'] = PhoBERT_paraphased_tomtat['paraphased_tomtat'].apply(lambda x: phobert_embedding(str(x)) if not pd.isna(x) else np.zeros((1, 768)))

PhoBERT_paraphased_tomtat['paraphased_tomtat'] = PhoBERT_paraphased_tomtat['paraphased_tomtat'].apply(lambda x: x[0])

embedding_dim = PhoBERT_paraphased_tomtat['paraphased_tomtat'][0].shape[0]
column_names = [f'embedding_dim_{i}' for i in range(embedding_dim)]

embedding_df = pd.DataFrame(PhoBERT_paraphased_tomtat['paraphased_tomtat'].to_list(), columns=column_names)

PhoBERT_paraphased_tomtat = pd.concat([PhoBERT_paraphased_tomtat.drop(columns=['paraphased_tomtat']), embedding_df], axis=1)

"""### Thuộc tính nâng cao"""

start_year_df = filtered_augmented_df.groupby('mssv')['namhoc'].min()

start_year_augmented_df = pd.merge(filtered_augmented_df, start_year_df, on='mssv')
start_year_augmented_df = start_year_augmented_df.rename(columns={'namhoc_x': 'namhoc', 'namhoc_y': 'namhoc_batdau'})

term_df = start_year_augmented_df
term_df['sohocky'] = term_df.apply(lambda x: (x['namhoc'] - x['namhoc_batdau']) * 2 + x['hocky'], axis=1)

group_course_df = term_df.copy()
group_course_df['nhomloaimh'] = group_course_df.apply(lambda x: 'CSN' if x['loaimh'] in ['CSNN', 'CSN', 'CNCS'] else 'CN' if x['loaimh'] in ['CNTC', 'CN', 'ĐA'] else 'TN' if x['loaimh'] in ['CĐTN', 'TTTN'] else x['loaimh'], axis=1)

pass_course_df = group_course_df.copy()
pass_course_df['hoanthanh'] = pass_course_df['diem'] >= 5
pass_course_df['hoanthanh'] = pass_course_df['hoanthanh'].astype(int)

group_course_df

"""### Lưu dữ liệu"""

processed_student_data = pass_course_df[['mssv', 'gioitinh', 'namhoc_batdau', 'khoa', 'hedt']]
processed_score_data = pass_course_df[['mssv', 'mamh', 'nhomloaimh', 'diem', 'trangthai', 'namhoc', 'hocky', 'sohocky', 'hoanthanh']]
processed_course_data = pass_course_df[['mamh', 'sotc', 'nganhmh', 'loaimh', 'nhomloaimh']]
processed_student_data = processed_student_data.drop_duplicates(subset=['mssv'])
processed_course_data = processed_course_data.drop_duplicates(subset=['mamh'])

processed_student_data.to_excel("/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Dataset/Processed/Processed Data With Raw Data/processed_student.xlsx")
processed_score_data.to_excel("/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Dataset/Processed/Processed Data With Raw Data/processed_score.xlsx")
processed_course_data.to_excel("/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Dataset/Processed/Processed Data With Raw Data/processed_course.xlsx")
PhoBERT_paraphased_tomtat.to_csv("/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Dataset/Processed/Processed Data With Raw Data/PhoBERT_paraphased_tomtat.csv")

onehot_encoding_student_data = pd.get_dummies(processed_student_data, columns=['gioitinh', 'khoa', 'hedt'])
onehot_encoding_score_data = pd.get_dummies(processed_score_data, columns=['nhomloaimh', 'trangthai'])
onehot_encoding_course_data = pd.get_dummies(processed_course_data, columns=['nganhmh', 'loaimh', 'nhomloaimh'])

onehot_encoding_student_data.to_excel("/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Dataset/Processed/Processed Data With Raw Data/onehot_student.xlsx")
onehot_encoding_score_data.to_excel("/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Dataset/Processed/Processed Data With Raw Data/onehot_score.xlsx")
onehot_encoding_course_data.to_excel("/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Dataset/Processed/Processed Data With Raw Data/onehot_course.xlsx")

"""### Thống kê môn học

1. Độ phổ biến
"""

subject_popularity = term_df.groupby(['mamh', 'namhoc','sohocky', 'khoa'])['mssv'].count()

subject_popularity = subject_popularity.reset_index().rename(columns={'mssv': 'sosv'})

total_students = term_df.groupby(['namhoc','sohocky', 'khoa'])['mssv'].count().reset_index().rename(columns={'mssv': 'tongsosv'})

subject_popularity = pd.merge(subject_popularity, total_students, on=['namhoc', 'sohocky', 'khoa'], how='left')

subject_popularity['dophobien'] = subject_popularity['sosv'] / subject_popularity['tongsosv']

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
subject_popularity['dophobien_scaled'] = scaler.fit_transform(subject_popularity[['dophobien']])

subject_popularity.to_excel("/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Dataset/Processed/Processed Data With Raw Data/subject_popularity.xlsx")

"""2. Kết quả học tập"""

subject_score = term_df.groupby(['mamh', 'namhoc', 'sohocky', 'khoa'])['diem'].mean()

subject_score = subject_score.reset_index().rename(columns={'diem': 'diemtb'})

subject_score['dothanhtich'] = subject_score['diemtb'] / 10

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
subject_score['dothanhtich_scaled'] = scaler.fit_transform(subject_score[['dothanhtich']])

subject_score.to_excel("/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Dataset/Processed/Processed Data With Raw Data/subject_score.xlsx")

"""3. Nhóm môn học"""

group_course = group_course_df.groupby(['khoa', 'namhoc', 'sohocky', 'nhomloaimh'])['mamh'].count() / group_course_df.groupby(['khoa', 'namhoc', 'sohocky', 'nhomloaimh'])['mssv'].nunique()

group_course = group_course.reset_index().rename(columns={0: 'somonhoc'})

group_course.to_excel("/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Dataset/Processed/Processed Data With Raw Data/group_course.xlsx")