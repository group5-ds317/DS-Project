{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0LkSK5pzdJW",
        "outputId": "41d9d61c-0540-4ab7-e220-c8dfad157f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRzq73iZyPFo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.preprocessing import normalize, OneHotEncoder, MinMaxScaler, OrdinalEncoder\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from pickle import encode_long\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hirj9mcHzeX6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "5f492c12-1651-4c27-c9d8-d5d834c88d6d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Dataset/Processed/Processed Data With Pre-Processed Data/processed_student.xlsx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c8fd2b0b97cf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocessed_student\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Dataset/Processed/Processed Data With Pre-Processed Data/processed_student.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprocessed_course\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Dataset/Processed/Processed Data With Pre-Processed Data/processed_course.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprocessed_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Dataset/Processed/Processed Data With Pre-Processed Data/processed_score.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msubject_popularity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Dataset/Processed/Processed Data With Pre-Processed Data/subject_popularity.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgroup_course\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Dataset/Processed/Processed Data With Pre-Processed Data/group_course.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Dataset/Processed/Processed Data With Pre-Processed Data/processed_student.xlsx'"
          ]
        }
      ],
      "source": [
        "processed_student = pd.read_excel('/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án môn học/Dataset/Processed/Processed Data With Pre-Processed Data/processed_student.xlsx', index_col=0)\n",
        "processed_course = pd.read_excel('/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án môn học/Dataset/Processed/Processed Data With Pre-Processed Data/processed_course.xlsx', index_col=0)\n",
        "processed_score = pd.read_excel('/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án môn học/Dataset/Processed/Processed Data With Pre-Processed Data/processed_score.xlsx', index_col=0)\n",
        "subject_popularity = pd.read_excel('/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án môn học/Dataset/Processed/Processed Data With Pre-Processed Data/subject_popularity.xlsx', index_col = 0)\n",
        "group_course = pd.read_excel('/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án môn học/Dataset/Processed/Processed Data With Pre-Processed Data/group_course.xlsx', index_col=0)\n",
        "group_sum_course = pd.read_excel('/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án môn học/Dataset/Processed/Processed Data With Pre-Processed Data/group_sum_course.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoNn2Nx5A-Kb"
      },
      "outputs": [],
      "source": [
        "class DataPreprocessor:\n",
        "    def __init__(self, student_df, score_df, course_df, subject_popularity_df, group_sum_course_df):\n",
        "        self.student = student_df\n",
        "        self.score = score_df\n",
        "        self.course = course_df\n",
        "        self.subject = subject_popularity_df\n",
        "        self.group_course = group_sum_course_df\n",
        "        self.ordinal_encoder = OrdinalEncoder()\n",
        "        self.nhomloaimh_mapping = None  # Store mapping between encoded and original labels\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # Encode student features\n",
        "        student_features = self.student[['mssv', 'khoa', 'namhoc_batdau']].copy()\n",
        "        one_hot_encoder = OneHotEncoder()\n",
        "        student_features_encoded = one_hot_encoder.fit_transform(student_features[['khoa']]).toarray()\n",
        "        self.student_features = pd.concat(\n",
        "            [student_features[['mssv', 'namhoc_batdau']], pd.DataFrame(student_features_encoded)], axis=1\n",
        "        )\n",
        "\n",
        "        # Normalize subject popularity features (instead of course features)\n",
        "        subject_features = self.subject[['mamh', 'dophobien']].copy()\n",
        "        scaler = MinMaxScaler()\n",
        "        subject_features['dophobien_scaled'] = scaler.fit_transform(subject_features[['dophobien']])\n",
        "        self.ordinal_encoder = self.ordinal_encoder.fit(self.course[['nhomloaimh']])\n",
        "\n",
        "        self.course['nhomloaimh_encoded'] = self.ordinal_encoder.transform(self.course[['nhomloaimh']])\n",
        "        # Save the mapping for decoding later\n",
        "        self.nhomloaimh_mapping = {\n",
        "        encoded: original\n",
        "        for encoded, original in zip(\n",
        "            self.course['nhomloaimh_encoded'], self.course['nhomloaimh']\n",
        "        )\n",
        "    }\n",
        "        self.course['nhomloaimh'] = self.course['nhomloaimh_encoded']\n",
        "        self.course_features = subject_features  # Use this as course-related features\n",
        "        self.course_features = pd.merge(subject_features, self.course[['mamh', 'nhomloaimh']], on='mamh', how='left')\n",
        "        # Prepare interaction data\n",
        "        interactions = self.score[['mssv', 'mamh', 'diem']].copy()\n",
        "        interactions['label'] = (interactions['diem'] >= 5).astype(int)  # 1 if passed, 0 if failed\n",
        "        return self.student_features, self.course_features, interactions\n",
        "\n",
        "    def get_group_course(self, faculty: str, year: int, term: int) -> pd.DataFrame:\n",
        "        group_course_result = self.group_course.loc[\n",
        "            (self.group_course['khoa'] == faculty) &\n",
        "            (self.group_course['sohocky'] == term),\n",
        "        :]\n",
        "        #If the result is empty try with year - 1:\n",
        "        if group_course_result.empty:\n",
        "          group_course_result = self.group_course.loc[\n",
        "            (self.group_course['khoa'] == faculty) &\n",
        "            (self.group_course['namhoc'] < year) &\n",
        "            (self.group_course['sohocky'] == term),\n",
        "        :]\n",
        "        # If still empty, consider all years\n",
        "        if group_course_result.empty:\n",
        "          group_course_result = self.group_course.loc[\n",
        "              (self.group_course['khoa'] == faculty) &\n",
        "              (self.group_course['sohocky'] == term),\n",
        "          :]\n",
        "        group_course_result = group_course_result[['nhomloaimh', 'somonhoc']]\n",
        "        group_course_result = group_course_result.groupby('nhomloaimh').mean()\n",
        "        group_course_result['somonhoc'] = group_course_result['somonhoc'].apply(lambda x: math.ceil(x))\n",
        "        return group_course_result.reset_index()\n",
        "\n",
        "    def get_faculty(self, mssv: str) -> str:\n",
        "        return str(self.student[self.student['mssv'] == mssv].loc[:, 'khoa'].values[0])\n",
        "\n",
        "    def get_year(self, mssv: str, term: int) -> int:\n",
        "        return int(self.score[(self.score['mssv'] == mssv) & (self.score['sohocky'] == term)]['namhoc'].values[0])\n",
        "\n",
        "    def get_highest_term(self, mssv: str) -> int:\n",
        "        return int(self.score[self.score['mssv'] == mssv]['sohocky'].max())\n",
        "\n",
        "class RecommendationSystem:\n",
        "    def __init__(self, mlp_model, preprocessor):\n",
        "        self.mlp_model = mlp_model\n",
        "        self.preprocessor = preprocessor\n",
        "\n",
        "    def recommend_by_group(self, mssv, term):\n",
        "        # Lấy thông tin về khoa, năm, và kỳ học cao nhất\n",
        "        faculty = self.preprocessor.get_faculty(mssv)\n",
        "        max_term = self.preprocessor.get_highest_term(mssv)\n",
        "        print(\"max term\", max_term)\n",
        "        print(\"term\", term)\n",
        "        if(max_term < term):\n",
        "          term = max_term\n",
        "        # else:\n",
        "        #   term = term - 1\n",
        "        year = self.preprocessor.get_year(mssv, term)\n",
        "\n",
        "        # Lấy thông tin nhóm môn học\n",
        "        group_course = self.preprocessor.get_group_course(faculty, year, term)\n",
        "        encoder = OrdinalEncoder()\n",
        "        group_course['nhomloaimh'] = encoder.fit_transform(group_course[['nhomloaimh']])\n",
        "        # Kết quả gợi ý cho từng nhóm\n",
        "        recommendations_by_group = {}\n",
        "        for group_id in group_course['nhomloaimh']:\n",
        "\n",
        "            # Decode the group name\n",
        "            group_name = self.preprocessor.nhomloaimh_mapping.get(float(group_id), f\"Nhóm {group_id}\")\n",
        "\n",
        "            # Lọc môn học theo nhóm\n",
        "            courses_in_group = self.preprocessor.course_features[\n",
        "                self.preprocessor.course_features['nhomloaimh'] == group_id\n",
        "            ]\n",
        "\n",
        "            # Lọc môn học chưa học\n",
        "            learned_courses = self.preprocessor.score[self.preprocessor.score['mssv'] == mssv]['mamh'].unique()\n",
        "\n",
        "            courses_not_learned = courses_in_group[~courses_in_group['mamh'].isin(learned_courses)]\n",
        "\n",
        "            # Remove duplicates based on the course ID ('mamh')\n",
        "            courses_not_learned = courses_not_learned.drop_duplicates(subset='mamh')\n",
        "\n",
        "            # Nếu không còn môn học nào, bỏ qua nhóm\n",
        "            if courses_not_learned.empty:\n",
        "                continue\n",
        "\n",
        "            # Tạo vector đặc trưng\n",
        "            student_feature = self.preprocessor.student_features[self.preprocessor.student_features['mssv'] == mssv]\n",
        "            student_vector = student_feature.drop(columns=['mssv']).values\n",
        "            course_ids = courses_not_learned['mamh'].values\n",
        "            course_features = courses_not_learned.drop(columns=['mamh']).values\n",
        "            student_course_vectors = np.hstack([np.repeat(student_vector, len(course_features), axis=0), course_features])\n",
        "            student_course_vectors = student_course_vectors.astype(np.float32)\n",
        "            # Dự đoán điểm số\n",
        "\n",
        "            expected_input_shape = self.mlp_model.model.input_shape[1]  # Get expected input size\n",
        "\n",
        "            # Check if the input data shape matches the expected shape\n",
        "            if student_course_vectors.shape[1] != expected_input_shape:\n",
        "                # If shapes do not match, try to reshape or adjust your data\n",
        "                # Here's a basic example of reshaping, assuming you need to drop a column:\n",
        "                student_course_vectors = student_course_vectors[:, :expected_input_shape]\n",
        "\n",
        "            scores = self.mlp_model.predict(student_course_vectors).flatten()\n",
        "            # Reranking dựa trên độ phổ biến của môn học và điểm của sinh viên\n",
        "            popularity_scores = courses_not_learned['dophobien_scaled'].values\n",
        "            avg_past_score = self.preprocessor.score[self.preprocessor.score['mssv'] == mssv]['diem'].mean()\n",
        "            final_scores = 0.6 * scores + 0.2 * popularity_scores + 0.2 * avg_past_score # Tùy chỉnh trọng số\n",
        "\n",
        "            # Gợi ý Top n môn học của nhóm\n",
        "            top_n = int(group_course.loc[group_course['nhomloaimh'] == group_id]['somonhoc'])\n",
        "            top_indices = np.argsort(final_scores)[::-1][:top_n]\n",
        "            recommended_courses = course_ids[top_indices]\n",
        "            # Lưu kết quả\n",
        "            recommendations_by_group[group_name] = recommended_courses\n",
        "        recommendations = []\n",
        "        for group_name, recommended_courses in recommendations_by_group.items():\n",
        "            recommendations.extend(recommended_courses)\n",
        "        recommendations = pd.Series(recommendations)\n",
        "        return recommendations\n",
        "\n",
        "class MLPModel:\n",
        "    def __init__(self, input_size, learning_rate=0.00001):\n",
        "        self.model = Sequential([\n",
        "            Dense(128, input_dim=input_size, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "        self.model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, epochs=10, batch_size=32):\n",
        "        self.model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=epochs, batch_size=batch_size, verbose=2\n",
        "        )\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNRFbyALBBYg"
      },
      "outputs": [],
      "source": [
        "# Khởi tạo Preprocessor và Recommendation System\n",
        "preprocessor = DataPreprocessor(processed_student, processed_score, processed_course, subject_popularity, group_course)\n",
        "student_features, course_features, interactions = preprocessor.prepare_data()\n",
        "# Merge student and course features\n",
        "merged_data = interactions.merge(student_features, on='mssv').merge(course_features, on='mamh')\n",
        "\n",
        "# Create input and labels\n",
        "X = merged_data.drop(columns=['mssv', 'mamh', 'diem', 'label']).values\n",
        "y = merged_data['label'].values\n",
        "\n",
        "# Split data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def evaluate(mssv: str, term: int, recommended_courses: list[str]):\n",
        "    if len(recommended_courses) == 0:\n",
        "      return 0, 0, 0\n",
        "    actual_courses = processed_score[(processed_score['mssv']==mssv) & (processed_score['sohocky'] == term)]['mamh'].unique()\n",
        "\n",
        "    true_positive_count = recommended_courses.isin(actual_courses).sum()\n",
        "    precision = true_positive_count / len(recommended_courses) if len(recommended_courses) > 0 else 0\n",
        "    recall = true_positive_count / len(actual_courses) if len(actual_courses) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return precision, recall, f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "random.seed(42)\n",
        "# Lọc các sinh viên bắt đầu học năm 2014\n",
        "student_2014 = processed_student[processed_student['namhoc_batdau'] == 2014]['mssv'].unique()\n",
        "\n",
        "terms = [2, 3, 4, 5]\n",
        "filtered_scores = processed_score[processed_score['sohocky'].isin(terms) & processed_score['mssv'].isin(student_2014)]\n",
        "\n",
        "student_with_all_terms = filtered_scores.groupby('mssv').filter(lambda x: set(x['sohocky']) == set(terms))['mssv'].unique()\n",
        "\n",
        "student_with_all_terms = list(student_with_all_terms)\n",
        "\n",
        "\n",
        "qualified_students = list(set(student_2014) & set(student_with_all_terms))\n",
        "selected_students = random.sample(qualified_students, 100)\n",
        "\n",
        "lr_list = [10e-8, 10e-7, 10e-6, 10e-5, 10e-4, 10e-3, 10e-2, 10e-1, 10e1, 10e2]\n",
        "epoch_range = 20\n",
        "batch_size_list = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
        "\n",
        "gridsearch_results = []\n",
        "for lr in tqdm(lr_list):\n",
        "  for epoch in range(1, 1 + epoch_range):\n",
        "    for batch_size in batch_size_list:\n",
        "      mlp = MLPModel(input_size=X_train.shape[1], learning_rate=lr)\n",
        "      mlp.train(X_train, y_train, X_val, y_val, epochs=epoch, batch_size=batch_size)\n",
        "      recommendation_system = RecommendationSystem(mlp, preprocessor)\n",
        "      evaluation_results = []\n",
        "      for i in range(2, 6):\n",
        "        for mssv in selected_students:\n",
        "            recommendations_by_group = recommendation_system.recommend_by_group(mssv, i)\n",
        "            precision, recall, f1_score = evaluate(mssv, i, recommendations_by_group)\n",
        "            evaluation_results.append({\n",
        "                'mssv': mssv,\n",
        "                'term': i,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1_score': f1_score\n",
        "            })\n",
        "\n",
        "      evaluation_df = pd.DataFrame(evaluation_results)\n",
        "      mean_precision_by_term = evaluation_df.groupby('term')['precision'].mean()\n",
        "      mean_precision = evaluation_df['precision'].mean()\n",
        "      mean_recall_by_term = evaluation_df.groupby('term')['recall'].mean()\n",
        "      mean_recall = evaluation_df['recall'].mean()\n",
        "      mean_f1_score_by_term = evaluation_df.groupby('term')['f1_score'].mean()\n",
        "      mean_f1_score = evaluation_df['f1_score'].mean()\n",
        "      gridsearch_results.append({\n",
        "          'learning rate': lr,\n",
        "          'epoch': epoch,\n",
        "          'batch size': batch_size,\n",
        "          'mean_precision': mean_precision,\n",
        "          'mean_recall': mean_recall,\n",
        "          'mean_f1_score': mean_f1_score\n",
        "      })\n",
        "gridsearch_df = pd.DataFrame(gridsearch_results)\n",
        "gridsearch_df.to_excel('/content/drive/MyDrive/Nhóm 5 - DS317.P11/Đồ án mô học/Gridsearch Finetune/Finetune With Pre-Processed Data/Baseline 5/gridsearch_results.xlsx')"
      ],
      "metadata": {
        "id": "EazgRkzOCjjT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}